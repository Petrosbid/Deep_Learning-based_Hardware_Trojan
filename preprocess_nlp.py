#
# preprocess_nlp.py
# (ูุงุฒ 2: ุฌูุนโุขูุฑ ุชูุงู ุฏุงุฏูโูุง ูุงุฒ 1 ุจุฑุง ุขููุฒุด NLP ู DL)
#
import os
import glob
import json
import time
from typing import List, Dict, Any, TextIO
from tqdm import tqdm

# --- ุชูุธูุงุช ---
DATASET_ROOTS = ["TRIT-TC", "TRIT-TS"]

# ูุงู ูุงูโูุง ุฎุฑูุฌ ุจู .jsonl ุชุบุฑ ฺฉุฑุฏ (JSON Lines)
CORPUS_OUTPUT_FILE = "corpus_ALL.jsonl"
LABELED_DATA_OUTPUT_FILE = "labeled_traces_ALL.jsonl"


# -----------------

def find_all_trace_files(root_folders: List[str]) -> List[str]:
    """
    ุชูุงู ูุงูโูุง _traces.json ุฑุง ุฏุฑ ุชูุงู ุฒุฑูพูุดูโูุง ูพุฏุง ูโฺฉูุฏ.
    """
    all_files = []
    print(f"๐ ุฏุฑ ุญุงู ุฌุณุชุฌู ุจุฑุง ูุงูโูุง _traces.json ุฏุฑ {root_folders}...")
    for root in root_folders:
        if not os.path.isdir(root):
            print(f"  โ๏ธ ูุดุฏุงุฑ: ูพูุดู '{root}' ุงูุช ูุดุฏ. ุงุฒ ุขู ุนุจูุฑ ูโฺฉูู.")
            continue

        search_pattern = os.path.join(root, '**', '*_traces.json')
        files_found = glob.glob(search_pattern, recursive=True)
        all_files.extend(files_found)

    return all_files


def build_comprehensive_datasets_streaming(
        trace_files: List[str],
        corpus_file_handle: TextIO,
        labeled_file_handle: TextIO
) -> int:
    """
    (ุจูููโุณุงุฒ ุดุฏู ุจุฑุง RAM)
    ูุงูโูุง JSON ุฑุง ูโุฎูุงูุฏ ู ุฏุงุฏูโูุง ุฑุง ุจู ุตูุฑุช ุฌุฑุงู ุฏุฑ ูุงูโูุง ุจุงุฒ ูโููุณุฏ.
    ุชุนุฏุงุฏ ฺฉู ุฑุฏุงุจโูุง ูพุฑุฏุงุฒุด ุดุฏู ุฑุง ุจุฑูโฺฏุฑุฏุงูุฏ.
    """
    total_traces_processed = 0

    # --- โจ ุงุณุชูุงุฏู ุงุฒ tqdm ุจุฑุง ููุงุฑ ูพุดุฑูุช โจ ---
    # desc="..." ูุชู ุงุณุช ฺฉู ฺฉูุงุฑ ููุงุฑ ูพุดุฑูุช ููุงุด ุฏุงุฏู ูโุดูุฏ
    print(f"Processing {len(trace_files)} files...")
    for filepath in tqdm(trace_files, desc="๐ Processing JSON files", unit="file"):

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                traces_in_file = json.load(f)

            if not isinstance(traces_in_file, list):
                tqdm.write(f"  โ๏ธ ูุดุฏุงุฑ: ูุงู {filepath} ูุฑูุช ูุณุช ูุฏุงุฑุฏ. ุนุจูุฑ ุดุฏ.")
                continue

            for item in traces_in_file:
                if 'trace' in item and 'label' in item and 'gate' in item:

                    # 1. ููุดุชู ุฌููู ุฏุฑ ูุงู corpus (ูุฑูุช jsonl)
                    # (ููุท ุฎูุฏ ุฌูููุ ุจุฏูู indent ุจุฑุง ุตุฑููโุฌู ุฏุฑ ูุถุง)
                    corpus_file_handle.write(json.dumps(item['trace']) + "\n")

                    # 2. ููุดุชู ุขุจุฌฺฉุช ฺฉุงูู ุฏุฑ ูุงู ุฏุงุฏูโูุง ุจุฑฺุณุจโุฏุงุฑ (ูุฑูุช jsonl)
                    labeled_file_handle.write(json.dumps(item) + "\n")

                    total_traces_processed += 1
                else:
                    tqdm.write(f"  โ๏ธ ูุดุฏุงุฑ: ุขุชู ูุนูุจ ุฏุฑ {filepath}. ุนุจูุฑ ุดุฏ.")

        except json.JSONDecodeError:
            tqdm.write(f"  โ ุฎุทุง: ูุงู {filepath} ุฎุฑุงุจ ุงุณุช (JSONDecodeError). ุนุจูุฑ ุดุฏ.")
        except Exception as e:
            tqdm.write(f"  โ ุฎุทุง: ุฎุทุง ูุงุดูุงุฎุชู ุฏุฑ {filepath}: {e}. ุนุจูุฑ ุดุฏ.")

    return total_traces_processed


def main():
    start_time = time.time()

    # 1. ูพุฏุง ฺฉุฑุฏู ุชูุงู ูุงูโูุง JSON
    all_json_files = find_all_trace_files(DATASET_ROOTS)

    if not all_json_files:
        print("โ ุฎุทุง: ูฺ ูุงู _traces.json ูพุฏุง ูุดุฏ. ุขุง ุงุณฺฉุฑูพุชโูุง ูุงุฒ 1 ุฑุง ุงุฌุฑุง ฺฉุฑุฏูโุงุฏุ")
        return

    print(f"โ {len(all_json_files)} ูุงู _traces.json ูพุฏุง ุดุฏ.")

    # 2. ุณุงุฎุช ุฏุชุงุณุชโูุง ุฌุงูุน (ุจู ุฑูุด ุฌุฑุงู)
    try:
        # ูุงูโูุง ุฎุฑูุฌ ุฑุง ุจุงุฒ ูโฺฉูู
        with open(CORPUS_OUTPUT_FILE, 'w', encoding='utf-8') as corpus_f, \
                open(LABELED_DATA_OUTPUT_FILE, 'w', encoding='utf-8') as labeled_f:

            print(f"๐พ ุฏุฑ ุญุงู ููุดุชู ุฎุฑูุฌโูุง ุฏุฑ {CORPUS_OUTPUT_FILE} ู {LABELED_DATA_OUTPUT_FILE}...")

            total_traces = build_comprehensive_datasets_streaming(all_json_files, corpus_f, labeled_f)

    except IOError as e:
        print(f"โ ุฎุทุง: ุงูฺฉุงู ููุดุชู ุฏุฑ ูุงูโูุง ุฎุฑูุฌ ูุฌูุฏ ูุฏุงุฑุฏ: {e}")
        return
    except Exception as e:
        print(f"โ ุฎุทุง ูุงุดูุงุฎุชู ุฏุฑ ุญู ูพุฑุฏุงุฒุด ุฌุฑุงู: {e}")
        return

    if total_traces == 0:
        print("โ ุฎุทุง: ูฺ ุฑุฏุงุจ (trace) ูุนุชุจุฑ ุจุฑุง ูพุฑุฏุงุฒุด ูพุฏุง ูุดุฏ.")
        return

    print("โ ูพุฑุฏุงุฒุด ุฌุฑุงู ุฏุงุฏูโูุง ุจุง ููููุช ุงูุฌุงู ุดุฏ.")

    # 3. ฺฏุฒุงุฑุด ููุง
    end_time = time.time()
    print("\n" + "=" * 50)
    print("๐ ูุงุฒ 2 (Preprocess NLP) ุจุง ููููุช ฺฉุงูู ุดุฏ")
    print("=" * 50)
    print(f"โฑ๏ธ ุฒูุงู ฺฉู: {end_time - start_time:.2f} ุซุงูู")
    print(f"๐ ุชุนุฏุงุฏ ฺฉู ูุงูโูุง JSON ูพุฑุฏุงุฒุด ุดุฏู: {len(all_json_files)}")
    print(f"๐ฌ ุชุนุฏุงุฏ ฺฉู ุฑุฏุงุจโูุง (ุฌููุงุช) ุฌูุนโุขูุฑ ุดุฏู: {total_traces}")
    print(f"RAM Usage: (ุจุณุงุฑ ฺฉูุ ุจู ูุทู ูพุฑุฏุงุฒุด ุฌุฑุงู)")
    print(f"๐ ุฎุฑูุฌโูุง:")
    print(f"  1. {CORPUS_OUTPUT_FILE} (ุจุฑุง ุขููุฒุด Net2Vec)")
    print(f"  2. {LABELED_DATA_OUTPUT_FILE} (ุจุฑุง ุขููุฒุด ุขุดฺฉุงุฑุณุงุฒ LSTM)")


if __name__ == "__main__":
    main()