#
# dataset.py
# (Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ: "Load all in RAM" + ÙÛŒÙ„ØªØ± "allowed_circuits_list")
#
import json
import torch
import numpy as np
from torch.utils.data import Dataset
from gensim.models import KeyedVectors
from tqdm import tqdm
import os
import io

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª ---
LABELED_DATA_FILE = "../jsonl_dataset/labeled_traces_BALANCED.jsonl"
EMBEDDING_FILE = "../Model/net2vec.vectors"
EMBEDDING_DIM = 100
LOGIC_LEVEL = 4
MAX_TRACE_LENGTH = (2 * LOGIC_LEVEL) - 1  # (7)


# -----------------

class TrojanDataset(Dataset):
    """
    (Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Ú©Ø§Ù…Ù„)
    1. ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø± __init__ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯ Ùˆ Ø¯Ø± RAM Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±Ø¯.
    2. Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù† allowed_circuits_list Ø±Ø§ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ù…ÛŒâ€ŒÙ¾Ø°ÛŒØ±Ø¯.
    """

    def __init__(self, data_file, embedding_file, allowed_circuits_list: set = None):
        """
        Ø§Ú¯Ø± allowed_circuits_list Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ØŒ
        ÙÙ‚Ø· Ø±Ø¯ÛŒØ§Ø¨ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¢Ù† Ù…Ø¯Ø§Ø±Ù‡Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.
        """
        self.data_file = data_file

        if not os.path.exists(data_file):
            raise FileNotFoundError(f"âŒ ÙØ§ÛŒÙ„ Ø¯ÛŒØªØ§Ø³Øª {data_file} ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        if not os.path.exists(embedding_file):
            raise FileNotFoundError(f"âŒ ÙØ§ÛŒÙ„ Embedding {embedding_file} ÛŒØ§ÙØª Ù†Ø´Ø¯.")

        print("--- 1. Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Net2Vec (Embeddings)...")
        # --- Ø§ØµÙ„Ø§Ø­ Ø§Ø´ØªØ¨Ø§Ù‡ ØªØ§ÛŒÙ¾ÛŒ ---
        self.embeddings = KeyedVectors.load_word2vec_format(embedding_file)
        # -------------------------
        self.zero_vector = np.zeros(EMBEDDING_DIM).astype(np.float32)
        print("âœ… ... Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯.")

        print(f"--- 2. Ø¯Ø± Ø­Ø§Ù„ Ø®ÙˆØ§Ù†Ø¯Ù† Ùˆ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† {data_file} (Ø¯Ø± RAM)...")

        self.data = []  # ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯

        with open(self.data_file, 'r', encoding='utf-8') as f:
            for line in tqdm(f, desc="ğŸ“Š Ø®ÙˆØ§Ù†Ø¯Ù† Ùˆ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù†", unit="L"):
                try:
                    line_stripped = line.strip()
                    if line_stripped:
                        item = json.loads(line_stripped)

                        # --- âœ¨âœ¨âœ¨ Ù…Ù†Ø·Ù‚ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† (Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù†Ø´Øª Ø¯Ø§Ø¯Ù‡) âœ¨âœ¨âœ¨ ---
                        if allowed_circuits_list is not None:
                            # Ø§Ú¯Ø± Ù„ÛŒØ³Øª ÙÛŒÙ„ØªØ± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŒ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù† Ú©Ù‡ Ø¢ÛŒØ§ Ù…Ø¯Ø§Ø± Ø§ÛŒÙ† Ø¢ÛŒØªÙ…
                            # Ø¯Ø± Ù„ÛŒØ³Øª Ù…Ø¬Ø§Ø² Ù‡Ø³Øª ÛŒØ§ Ù†Ù‡
                            if item.get('circuit') in allowed_circuits_list:
                                self.data.append(item)
                        else:
                            # Ø§Ú¯Ø± Ù„ÛŒØ³Øª ÙÛŒÙ„ØªØ±ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ØŒ Ù‡Ù…Ù‡ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
                            self.data.append(item)
                        # --- Ù¾Ø§ÛŒØ§Ù† Ù…Ù†Ø·Ù‚ ÙÛŒÙ„ØªØ± ---

                except (json.JSONDecodeError, KeyError):
                    # Ø§Ø² Ø®Ø·ÙˆØ· Ø®Ø±Ø§Ø¨ ÛŒØ§ ÙØ§Ù‚Ø¯ Ú©Ù„ÛŒØ¯ 'circuit' Ø±Ø¯ Ø´Ùˆ
                    tqdm.write(f"âš ï¸ Ø®Ø· Ø®Ø±Ø§Ø¨ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯: {line_stripped[:50]}...")

        self.total_samples = len(self.data)
        if self.total_samples == 0:
            print("âš ï¸ Ù‡Ø´Ø¯Ø§Ø±: 0 Ù†Ù…ÙˆÙ†Ù‡ Ù¾Ø³ Ø§Ø² ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯. Ø¢ÛŒØ§ Ù†Ø§Ù… Ù…Ø¯Ø§Ø±Ù‡Ø§ Ø¯Ø±Ø³Øª Ø§Ø³ØªØŸ")
        else:
            print(f"âœ… ... {self.total_samples:,} Ù†Ù…ÙˆÙ†Ù‡ (ÙÛŒÙ„ØªØ± Ø´Ø¯Ù‡) Ø¯Ø± RAM Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯.")
        print("--- Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ ---")

    def __len__(self):
        return self.total_samples

    def __getitem__(self, idx):
        """
        ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ ÙˆØ§Ø­Ø¯ Ø±Ø§ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø§Ø² RAM Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯ (Ø¨Ø³ÛŒØ§Ø± Ø³Ø±ÛŒØ¹).
        """
        item = self.data[idx]
        trace_words = item['trace']
        label = item['label']
        gate = item['gate']

        trace_tensor_data = np.zeros((MAX_TRACE_LENGTH, EMBEDDING_DIM), dtype=np.float32)

        for i, word in enumerate(trace_words):
            if i >= MAX_TRACE_LENGTH:
                break
            if word in self.embeddings:
                trace_tensor_data[i] = self.embeddings[word]

        return {
            "trace_tensor": torch.tensor(trace_tensor_data, dtype=torch.float32),
            "label": torch.tensor(label, dtype=torch.long),
            "gate": gate
        }


# ... (Ø¨Ø®Ø´ __main__ Ø¨Ø±Ø§ÛŒ ØªØ³Øª) ...
if __name__ == "__main__":
    print("--- ğŸ§ª Ø´Ø±ÙˆØ¹ ØªØ³Øª TrojanDataset (Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡) ---")
    try:
        # ØªØ³Øª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¨Ø¯ÙˆÙ† ÙÛŒÙ„ØªØ±
        print("\n--- ØªØ³Øª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ (Ø¨Ø¯ÙˆÙ† ÙÛŒÙ„ØªØ±) ---")
        dataset_full = TrojanDataset(LABELED_DATA_FILE, EMBEDDING_FILE)
        print(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§: {len(dataset_full):,}")

        # ØªØ³Øª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¨Ø§ ÙÛŒÙ„ØªØ± (ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ù…Ø¯Ø§Ø±ÛŒ Ø¨Ù‡ Ù†Ø§Ù… 'c2670_T001' ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯)
        print("\n--- ØªØ³Øª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ (Ø¨Ø§ ÙÛŒÙ„ØªØ±) ---")
        dataset_filtered = TrojanDataset(LABELED_DATA_FILE, EMBEDDING_FILE, allowed_circuits_list={'c2670_T001'})
        print(f"Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ ÙÛŒÙ„ØªØ± Ø´Ø¯Ù‡: {len(dataset_filtered):,}")

        sample = dataset_filtered[0]
        print("\n--- Ù†Ù…ÙˆÙ†Ù‡ Ø§ÙˆÙ„ (ÙÛŒÙ„ØªØ± Ø´Ø¯Ù‡) ---")
        print(f"Gate: {sample['gate']}")
        print(f"Label: {sample['label']}")
        print(f"Tensor Shape: {sample['trace_tensor'].shape}")

        print("\nâœ… Ø§Ø³Ú©Ø±ÛŒÙ¾Øª dataset (Ù†Ù‡Ø§ÛŒÛŒ) Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯.")

    except FileNotFoundError as e:
        print(e)
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ Ø¯Ø± Ø­ÛŒÙ† ØªØ³Øª: {e}")