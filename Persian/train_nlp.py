import os
import json
import gensim
import time
import logging

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª ---
CORPUS_FILE = "../jsonl_dataset/corpus_ALL.jsonl"

MODEL_OUTPUT_FILE = "../Model/net2vec.vectors"

# Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„ØŒ Ù…Ø·Ø§Ø¨Ù‚ Ø¨Ø§ Ù…Ù‚Ø§Ù„Ù‡
EMBEDDING_DIM = 100  # (N=100)
NEGATIVE_SAMPLES = 5  # (K=5)
WINDOW_SIZE = 9  # (2 * logic_level=5) - 1. ÛŒÚ© Ù¾Ù†Ø¬Ø±Ù‡ Ø¨Ø²Ø±Ú¯ Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
MIN_WORD_COUNT = 1  # Ø­Ø¯Ø§Ù‚Ù„ ØªØ¹Ø¯Ø§Ø¯ ØªÚ©Ø±Ø§Ø± ÛŒÚ© Ú©Ù„Ù…Ù‡ Ø¨Ø±Ø§ÛŒ Ù„Ø­Ø§Ø¸ Ø´Ø¯Ù†
WORKERS = os.cpu_count() - 2  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÙ…Ø§Ù… Ù‡Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ CPU Ø¨Ø¬Ø² 2 ØªØ§

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)


class CorpusStreamer:
    """
    Ú©Ù„Ø§Ø³ÛŒ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù† 48 Ù…ÛŒÙ„ÛŒÙˆÙ† Ø®Ø· Ø¨Ø¯ÙˆÙ† Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø± RAM.
    Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ ÙØ§ÛŒÙ„ .jsonl Ø±Ø§ Ø®Ø· Ø¨Ù‡ Ø®Ø· Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯ Ùˆ Ù‡Ø± Ø®Ø· Ø±Ø§ yield Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """

    def __init__(self, filepath):
        self.filepath = filepath
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"ÙØ§ÛŒÙ„ Corpus ÛŒØ§ÙØª Ù†Ø´Ø¯: {filepath}")

    def __iter__(self):
        print("\n--- ğŸ§  Ø´Ø±ÙˆØ¹ Ø®ÙˆØ§Ù†Ø¯Ù† Ø¬Ø±ÛŒØ§Ù†ÛŒ Corpus... (Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø¨Ø§Ø´Ø¯) ---")
        try:
            with open(self.filepath, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        yield json.loads(line)
                    except json.JSONDecodeError:
                        print(f"Warning: Skipping malformed line: {line}")
        except Exception as e:
            print(f"Error reading corpus file: {e}")
            raise


def train_net2vec_model(corpus_filepath):
    """
    Ù…Ø¯Ù„ Net2Vec (Word2Vec) Ø±Ø§ Ø¢Ù…ÙˆØ²Ø´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
    """
    start_time = time.time()

    sentences_stream = CorpusStreamer(corpus_filepath)

    print(f"--- ğŸ‹ï¸ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Word2Vec ---")
    print(f"Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:")
    print(f"  Dimensions (vector_size): {EMBEDDING_DIM}")
    print(f"  Algorithm (sg): 1 (Skip-gram)")
    print(f"  Negative Sampling: {NEGATIVE_SAMPLES}")
    print(f"  Workers (CPU Cores): {WORKERS}")
    print("Ø§ÛŒÙ† ÙØ±Ø¢ÛŒÙ†Ø¯ Ú†Ù†Ø¯ÛŒÙ† Ø³Ø§Ø¹Øª Ø·ÙˆÙ„ Ø®ÙˆØ§Ù‡Ø¯ Ú©Ø´ÛŒØ¯ Ùˆ Ø§Ø² CPU Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯...")

    model = gensim.models.Word2Vec(
        sentences=sentences_stream,
        vector_size=EMBEDDING_DIM,
        sg=1,
        negative=NEGATIVE_SAMPLES,
        window=WINDOW_SIZE,
        min_count=MIN_WORD_COUNT,
        workers=WORKERS,
        epochs=5
    )

    end_time = time.time()
    print(f"\n--- âœ… Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ú©Ø§Ù…Ù„ Ø´Ø¯ ---")
    print(f"Ø²Ù…Ø§Ù† Ø¢Ù…ÙˆØ²Ø´: {(end_time - start_time) / 60:.2f} Ø¯Ù‚ÛŒÙ‚Ù‡ (ÛŒØ§ {(end_time - start_time) / 3600:.2f} Ø³Ø§Ø¹Øª)")

    # 3. Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„
    try:
        print(f"ğŸ’¾ Ø¯Ø± Ø­Ø§Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ Ø¯Ø± {MODEL_OUTPUT_FILE}...")
        model.wv.save_word2vec_format(MODEL_OUTPUT_FILE)
        print("âœ… ... Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„: {e}")

    vocab_size = len(model.wv.index_to_key)
    print(f"\n--- ğŸ“Š Ù†ØªØ§ÛŒØ¬ Ù…Ø¯Ù„ ---")
    print(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ú©Ù„Ù…Ø§Øª (PCPs) ÛŒÚ©ØªØ§ Ø¯Ø± Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ: {vocab_size}")
    if vocab_size > 0:
        print("10 Ú©Ù„Ù…Ù‡ Ø§ÙˆÙ„ Ø¯Ø± Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ:")
        for i, word in enumerate(model.wv.index_to_key[:10]):
            print(f"  {i + 1}. {word}")


def main():
    if not os.path.exists(CORPUS_FILE):
        print(f"âŒ Ø®Ø·Ø§: ÙØ§ÛŒÙ„ {CORPUS_FILE} ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        print("Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª preprocess_nlp.py Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯.")
        return

    train_net2vec_model(CORPUS_FILE)
    print("\nğŸ ÙØ§Ø² 2 (Ø¢Ù…ÙˆØ²Ø´ NLP) Ú©Ø§Ù…Ù„ Ø´Ø¯.")
    print(f"Ù…Ø±Ø­Ù„Ù‡ Ø¨Ø¹Ø¯ÛŒ: Ø³Ø§Ø®Øª Ø¯ÛŒØªØ§Ø³Øª Ù…ØªØ¹Ø§Ø¯Ù„ (Downsampling) Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ø¢Ø´Ú©Ø§Ø±Ø³Ø§Ø² LSTM.")


if __name__ == "__main__":
    main()