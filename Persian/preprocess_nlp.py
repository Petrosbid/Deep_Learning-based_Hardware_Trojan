import os
import glob
import json
import time
from typing import List, Dict, Any, TextIO
from tqdm import tqdm

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª ---
DATASET_ROOTS = ["../Dataset/TRIT-TC", "../Dataset/TRIT-TS"]

# Ù†Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ù‡ .jsonl ØªØºÛŒÛŒØ± Ú©Ø±Ø¯ (JSON Lines)
CORPUS_OUTPUT_FILE = "../jsonl_dataset/corpus_ALL.jsonl"
LABELED_DATA_OUTPUT_FILE = "../jsonl_dataset/labeled_traces_ALL.jsonl"


# -----------------

def find_all_trace_files(root_folders: List[str]) -> List[str]:
    """
    ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ _traces.json Ø±Ø§ Ø¯Ø± ØªÙ…Ø§Ù… Ø²ÛŒØ±Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    all_files = []
    print(f"ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ _traces.json Ø¯Ø± {root_folders}...")
    for root in root_folders:
        if not os.path.isdir(root):
            print(f"  âš ï¸ Ù‡Ø´Ø¯Ø§Ø±: Ù¾ÙˆØ´Ù‡ '{root}' ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø§Ø² Ø¢Ù† Ø¹Ø¨ÙˆØ± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….")
            continue

        search_pattern = os.path.join(root, '**', '*_traces.json')
        files_found = glob.glob(search_pattern, recursive=True)
        all_files.extend(files_found)

    return all_files


def build_comprehensive_datasets_streaming(
        trace_files: List[str],
        corpus_file_handle: TextIO,
        labeled_file_handle: TextIO
) -> int:
    """
    (Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ RAM)
    ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ JSON Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯ Ùˆ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¬Ø±ÛŒØ§Ù†ÛŒ Ø¯Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø² Ù…ÛŒâ€ŒÙ†ÙˆÛŒØ³Ø¯.
    ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø±Ø¯ÛŒØ§Ø¨ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    """
    total_traces_processed = 0

    print(f"Processing {len(trace_files)} files...")
    for filepath in tqdm(trace_files, desc="ğŸ“Š Processing JSON files", unit="file"):

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                traces_in_file = json.load(f)

            if not isinstance(traces_in_file, list):
                tqdm.write(f"  âš ï¸ Ù‡Ø´Ø¯Ø§Ø±: ÙØ§ÛŒÙ„ {filepath} ÙØ±Ù…Øª Ù„ÛŒØ³Øª Ù†Ø¯Ø§Ø±Ø¯. Ø¹Ø¨ÙˆØ± Ø´Ø¯.")
                continue

            for item in traces_in_file:
                if 'trace' in item and 'label' in item and 'gate' in item:

                    corpus_file_handle.write(json.dumps(item['trace']) + "\n")

                    labeled_file_handle.write(json.dumps(item) + "\n")

                    total_traces_processed += 1
                else:
                    tqdm.write(f"  âš ï¸ Ù‡Ø´Ø¯Ø§Ø±: Ø¢ÛŒØªÙ… Ù…Ø¹ÛŒÙˆØ¨ Ø¯Ø± {filepath}. Ø¹Ø¨ÙˆØ± Ø´Ø¯.")

        except json.JSONDecodeError:
            tqdm.write(f"  âŒ Ø®Ø·Ø§: ÙØ§ÛŒÙ„ {filepath} Ø®Ø±Ø§Ø¨ Ø§Ø³Øª (JSONDecodeError). Ø¹Ø¨ÙˆØ± Ø´Ø¯.")
        except Exception as e:
            tqdm.write(f"  âŒ Ø®Ø·Ø§: Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ Ø¯Ø± {filepath}: {e}. Ø¹Ø¨ÙˆØ± Ø´Ø¯.")

    return total_traces_processed


def main():
    start_time = time.time()

    all_json_files = find_all_trace_files(DATASET_ROOTS)

    if not all_json_files:
        print("âŒ Ø®Ø·Ø§: Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ _traces.json Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. Ø¢ÛŒØ§ Ø§Ø³Ú©Ø±ÛŒÙ¾Øªâ€ŒÙ‡Ø§ÛŒ ÙØ§Ø² 1 Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒØ¯ØŸ")
        return

    print(f"âœ… {len(all_json_files)} ÙØ§ÛŒÙ„ _traces.json Ù¾ÛŒØ¯Ø§ Ø´Ø¯.")

    try:
        with open(CORPUS_OUTPUT_FILE, 'w', encoding='utf-8') as corpus_f, \
                open(LABELED_DATA_OUTPUT_FILE, 'w', encoding='utf-8') as labeled_f:

            print(f"ğŸ’¾ Ø¯Ø± Ø­Ø§Ù„ Ù†ÙˆØ´ØªÙ† Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± {CORPUS_OUTPUT_FILE} Ùˆ {LABELED_DATA_OUTPUT_FILE}...")

            total_traces = build_comprehensive_datasets_streaming(all_json_files, corpus_f, labeled_f)

    except IOError as e:
        print(f"âŒ Ø®Ø·Ø§: Ø§Ù…Ú©Ø§Ù† Ù†ÙˆØ´ØªÙ† Ø¯Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯: {e}")
        return
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ Ø¯Ø± Ø­ÛŒÙ† Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¬Ø±ÛŒØ§Ù†ÛŒ: {e}")
        return

    if total_traces == 0:
        print("âŒ Ø®Ø·Ø§: Ù‡ÛŒÚ† Ø±Ø¯ÛŒØ§Ø¨ÛŒ (trace) Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.")
        return

    print("âœ… Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¬Ø±ÛŒØ§Ù†ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")

    # 3. Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ
    end_time = time.time()
    print("\n" + "=" * 50)
    print("ğŸ ÙØ§Ø² 2 (Preprocess NLP) Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ú©Ø§Ù…Ù„ Ø´Ø¯")
    print("=" * 50)
    print(f"â±ï¸ Ø²Ù…Ø§Ù† Ú©Ù„: {end_time - start_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")
    print(f"ğŸ“‚ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ JSON Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡: {len(all_json_files)}")
    print(f"ğŸ’¬ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø±Ø¯ÛŒØ§Ø¨ÛŒâ€ŒÙ‡Ø§ (Ø¬Ù…Ù„Ø§Øª) Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯Ù‡: {total_traces}")
    print(f"RAM Usage: (Ø¨Ø³ÛŒØ§Ø± Ú©Ù…ØŒ Ø¨Ù‡ Ù„Ø·Ù Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¬Ø±ÛŒØ§Ù†ÛŒ)")
    print(f"ğŸ“Š Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§:")
    print(f"  1. {CORPUS_OUTPUT_FILE} (Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Net2Vec)")
    print(f"  2. {LABELED_DATA_OUTPUT_FILE} (Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¢Ø´Ú©Ø§Ø±Ø³Ø§Ø² LSTM)")


if __name__ == "__main__":
    main()